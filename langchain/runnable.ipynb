{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\"../.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Harrison worked at Kensho.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "vectorstore = FAISS.from_texts(\n",
    "    [\"harrison worked at kensho\"], embedding=OpenAIEmbeddings()\n",
    ")\n",
    "retriever = vectorstore.as_retriever()\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "model = ChatOpenAI()\n",
    "\n",
    "retrieval_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "retrieval_chain.invoke(\"where did harrison work?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    ")\n",
    "\n",
    "prompt_value = prompt_chain.invoke(\"where did harrison work?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content=\"Answer the question based only on the following context:\\n[Document(page_content='harrison worked at kensho')]\\n\\nQuestion: where did harrison work?\\n\")]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_value.to_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "InputType = retrieval_chain.InputType\n",
    "OutputType = retrieval_chain.OutputType\n",
    "def f(a: InputType) -> OutputType:\n",
    "    return \"123\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval_chain.InputType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"title\": \"RunnableSequence\", \"description\": \"Sequence of Runnables, where the output of each is the input of the next.\\\\n\\\\nRunnableSequence is the most important composition operator in LangChain as it is\\\\nused in virtually every chain.\\\\n\\\\nA RunnableSequence can be instantiated directly or more commonly by using the `|`\\\\noperator where either the left or right operands (or both) must be a Runnable.\\\\n\\\\nAny RunnableSequence automatically supports sync, async, batch.\\\\n\\\\nThe default implementations of `batch` and `abatch` utilize threadpools and\\\\nasyncio gather and will be faster than naive invocation of invoke or ainvoke\\\\nfor IO bound Runnables.\\\\n\\\\nBatching is implemented by invoking the batch method on each component of the\\\\nRunnableSequence in order.\\\\n\\\\nA RunnableSequence preserves the streaming properties of its components, so if all\\\\ncomponents of the sequence implement a `transform` method -- which\\\\nis the method that implements the logic to map a streaming input to a streaming\\\\noutput -- then the sequence will be able to stream input to output!\\\\n\\\\nIf any component of the sequence does not implement transform then the\\\\nstreaming will only begin after this component is run. If there are\\\\nmultiple blocking components, streaming begins after the last one.\\\\n\\\\nPlease note: RunnableLambdas do not support `transform` by default! So if\\\\n    you need to use a RunnableLambdas be careful about where you place them in a\\\\n    RunnableSequence (if you need to use the .stream()/.astream() methods).\\\\n\\\\n    If you need arbitrary logic and need streaming, you can subclass\\\\n    Runnable, and implement `transform` for whatever logic you need.\\\\n\\\\nHere is a simple example that uses simple functions to illustrate the use of\\\\nRunnableSequence:\\\\n\\\\n    .. code-block:: python\\\\n\\\\n        from langchain_core.runnables import RunnableLambda\\\\n\\\\n        def add_one(x: int) -> int:\\\\n            return x + 1\\\\n\\\\n        def mul_two(x: int) -> int:\\\\n            return x * 2\\\\n\\\\n        runnable_1 = RunnableLambda(add_one)\\\\n        runnable_2 = RunnableLambda(mul_two)\\\\n        sequence = runnable_1 | runnable_2\\\\n        # Or equivalently:\\\\n        # sequence = RunnableSequence(first=runnable_1, last=runnable_2)\\\\n        sequence.invoke(1)\\\\n        await sequence.ainvoke(1)\\\\n\\\\n        sequence.batch([1, 2, 3])\\\\n        await sequence.abatch([1, 2, 3])\\\\n\\\\nHere\\'s an example that uses streams JSON output generated by an LLM:\\\\n\\\\n    .. code-block:: python\\\\n\\\\n        from langchain_core.output_parsers.json import SimpleJsonOutputParser\\\\n        from langchain_openai import ChatOpenAI\\\\n\\\\n        prompt = PromptTemplate.from_template(\\\\n            \\'In JSON format, give me a list of {topic} and their \\'\\\\n            \\'corresponding names in French, Spanish and in a \\'\\\\n            \\'Cat Language.\\'\\\\n        )\\\\n\\\\n        model = ChatOpenAI()\\\\n        chain = prompt | model | SimpleJsonOutputParser()\\\\n\\\\n        async for chunk in chain.astream({\\'topic\\': \\'colors\\'}):\\\\n            print(\\'-\\')  # noqa: T201\\\\n            print(chunk, sep=\\'\\', flush=True)  # noqa: T201\", \"type\": \"object\", \"properties\": {\"name\": {\"title\": \"Name\", \"type\": \"string\"}, \"first\": {\"title\": \"First\", \"allOf\": [{\"type\": \"array\", \"items\": [{}, {}]}]}, \"middle\": {\"title\": \"Middle\", \"type\": \"array\", \"items\": {\"allOf\": [{\"type\": \"array\", \"items\": [{}, {}]}]}}, \"last\": {\"title\": \"Last\", \"allOf\": [{\"type\": \"array\", \"items\": [{}, {}]}]}}, \"required\": [\"first\", \"last\"]}'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval_chain.schema_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
